{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#using-generative-ai-to-generate-information-for-an-application","title":"Using generative AI to generate information for an application","text":"<p>Welcome to our lab! In this lab we'll be using watsonx to build an application which leverages generative AI to generate information for the app. The goals of this lab are to:</p> <ul> <li>Learn how to use generative AI by prompt engineering Large Language Models (LLMs) using watsonx.ai prompt lab</li> <li>Develop an application which uses the LLM (from prompt engineering) to generate relevant information</li> <li>Understand the value of AI in your applications</li> </ul> <p>Note: The application we are going to create is a travel chatbot app and we will use watsonx Assistant to help simplify making the chatbot.</p>"},{"location":"#about-these-labs","title":"About these labs","text":"<p>The introductory page of the lab is broken down into the following sections:</p> <ul> <li>Agenda</li> <li>Technology Used</li> <li>Credits</li> </ul>"},{"location":"#agenda","title":"Agenda","text":"Lab 0: Pre-work Pre-work for the project Lab 1: Using prompt engineering to identify the LLM for your application Learn how to use generative AI Lab 2: Create your application which uses LLM to generate information Learn how to create an application that uses generative AI"},{"location":"#technology-used","title":"Technology used","text":"<ul> <li>IBM Cloud</li> <li>watsonx.ai</li> <li>watsonx Assistant</li> </ul>"},{"location":"#credits","title":"Credits","text":"<ul> <li>Martin Hickey</li> <li>Tim Robinson</li> </ul>"},{"location":"lab-1/","title":"Using Prompt Engineering to Identify the LLM for your Application","text":"<p>Generative AI refers to deep-learning models that can generate high-quality text, images, and other content based on the data they were trained on. A Large Language Model (LLM) is a type of language model notable for its ability to achieve general-purpose language understanding and generation. </p> <p>The goal of this lab is to show how you can use prompt engineering with LLMs in order to elicit more accurate, relevant, and context-aware responses related to travel information on countries. We'll then leverage that information when building the application in Lab 2.</p> <p>Note: The following images show actual results from watsonx.ai prompt lab. The slight gray text is what we provided to the model.  The blue highlighted text is how the model responded. Be aware that the outputs shown here may not exactly resemble the outputs you receive when using the prompt lab.</p>"},{"location":"lab-1/#steps","title":"Steps","text":""},{"location":"lab-1/#step-1-getting-started","title":"Step 1. Getting started","text":"<p>When you open up watsonx.ai prompt lab and click the <code>Freeform</code> mode-option, this is what you will be shown. The large central text area is the prompt editor. On the right-side, you can display the model parameters that you can use to optimize how the model responds to your prompt. On the bottom-left is a summary of the number of tokens used by your prompt during execution. For more information on prompt engineering, see watsonx prompt lab.</p> <p>Note: The models provided in the prompt lab are foundation models which are hosted on IBM cloud so we can inference or call them. LLMs are a type of foundation model which we will use in this lab.</p>"},{"location":"lab-1/#step-2-first-prompt","title":"Step 2. First prompt","text":"<p>When starting with the initial prompt, it's best to just try something quickly and if it doesn't give you the result you want, then go ahead and improve it over time. We will do this in a step by step process.</p> <p>Let's start with:</p> <ul> <li>Model: <code>flan-t5-xxl-11b</code></li> <li>Prompt text: <code>I am thinking of travelling to Thailand.</code></li> </ul> <p>This produces the following output when we call the model (in other words, click on the \"Generate\" button):</p> <p></p> <p>This however returns a response which is not very useful, as we don't get any information on Thailand. This is analgous to questioning a person about a particular topic. The more open ended the questions, then the more generic the answers will be. However, the more closed the questions, then the more specific the answers. We therefore need to go again and provide more specfic context in the prompt to help direct the model to generate information about Thailand.</p>"},{"location":"lab-1/#step-3-being-more-direct","title":"Step 3. Being more direct","text":"<p>Let's be more direct in our prompt this time:</p> <ul> <li>Prompt text: <code>I am thinking of travelling to Thailand. Tell me about Thailand.</code></li> </ul> <p>This produces the following output when we call the model:</p> <p></p> <p>This is more promising as we are now getting some information on Thailand. However, it seems to finish mid-sentence. The summary of the number of tokens (on the bottom-left) provides the following reason: <code>Stop reason: Max tokens parameter reached</code>. </p> <p>This means we do not have enough tokens to process the request. Tokens represent the smallest entity which is handled by the model architecture. For our discussion in this lab, we will view tokens as equivalent to words (see Tokens and tokenization for a more exact definition). We therefore need to increase our \"Max tokens\".</p>"},{"location":"lab-1/#step-4-tinkering-with-model-parameters","title":"Step 4. Tinkering with model parameters","text":"<p>When we increase <code>Max tokens</code> to <code>200</code> and run with the same prompt again, we will get output similar to the following:</p> <p></p> <p>This time it finished the sentence and the summary of the number of tokens reason reads as follows: <code>Stop reason: End of sequence token encountered</code>. All good from that aspect.</p> <p>Every time we query the model however, it will return the same answer. This is because we are using <code>Greedy</code> decoding. This style of decoding asks the model to return what it thinks is the best or statistically the most accurate response always. Let's change <code>Decoding</code> to <code>Sampling</code> and see what the model returns with the same prompt each time.</p> <p>You will now see different responses similar to the following:</p> <p></p> <p>Variety is the spice of life! There are more parameters which you can configure the model differently with but we will not cover them in this lab. See watsonx prompt lab for more details on parameters and how to use them.</p>"},{"location":"lab-1/#step-5-getting-more-specific","title":"Step 5. Getting more specific","text":"<p>We have played around with the prompt text and the model parameters which has guided the model to return information on Thailand. It is however generic information and for the design of our travel app we would like to be able to provide information tailored to users interests. Let's therefore update the prompt text as follows to show information on water sports and food: <code>I am thinking of travelling to Thailand. I like water sports and food. Tell me about Thailand.</code>.</p> <p></p> <p>This response is very limited and it should be more informative to be useful in your application. We have tried different prompts and parameters, but we still don't have the relevant information we require. Maybe the model we are using has limitations for our requirements. Is it time therefore to check out out a different model?</p>"},{"location":"lab-1/#step-6-checking-out-other-models","title":"Step 6. Checking out other models","text":"<p>watsonx.ai prompt lab provides information cards on the models it supports. The card can provide information like:</p> <ul> <li>Provider and source of the model</li> <li>Tasks the model is best suited to</li> <li>How to tune the model</li> <li>White paper it is based on</li> <li>Bias, risks, and limitations</li> </ul> <p>You can access the information on the models by clicking on the dropdown alongside the model name and selecting \"View all foundation models\" as follows:</p> <p></p> <p>Here is list of the models currently supported on the free tier prompt lab:</p> <p></p> <p>Click on the <code>llama-2-70b-chat</code> model to look at its information card:</p> <p></p> <p>There is an interesting piece of information highlighted which states that the model is \"optimized for dialogue use cases\". As we are trying to find a model to question for information, it looks like this model might be a good fit. Click on \"Select model\" buton and let's try the model out to see if it returns more information on Thailand.</p>"},{"location":"lab-1/#step-7-using-a-different-model","title":"Step 7. Using a different model","text":"<p>We have now selected the <code>llama-2-70b-chat</code> model, and using the same prompt and parameters, let's see what it returns.</p> <p></p> <p>This definitely looks like an improvement but it looks like the response was cut off, similar to a previous step. This is confirmed by <code>Stop reason: Max tokens parameter reached</code> comment in the summary of the number of tokens. As tokens increase, so does the cost of calling a model. This time, instead of just increasing the token size, it would be great if we had a less expensive alternative. This is what we will try in the next section.</p>"},{"location":"lab-1/#step-8-adding-limits-to-the-prompt","title":"Step 8. Adding limits to the prompt","text":"<p>We will alter the prompt text by adding a limit to the response returned. In this way, we tell the model to return the response within the limit we define. The prompt text will now be altered to look like this: <code>I am thinking of travelling to Thailand. I like water sports and food. Give me 5 sentences on Thailand.</code>.</p> <p></p> <p>Finally, we have a response from our query on Thailand that is useful, informative and tailored to a user's preferences. It is also within our max token size. We have a result!</p>"},{"location":"lab-1/#conclusion-and-next-steps","title":"Conclusion and next steps","text":"<p>There is an alternative to training and creating a new model to fit your needs, and that is prompt tuning of a LLM. As you have seen in the lab we just did, it requires iterative testing and tweaking to find the model and prompt that suits your requirements. This is a simple example we are using but the same principles apply to other scenarios. It may require more context in the prompt, including using examples (i.e. shot prompting) to guide the response.</p> <p>It was mentioned previously that the models are hosted on IBM Cloud and are inferenced/called when we click the \"Generate\" button. You can check out the REST API that is called on the model by clicking on the \"View code\" dropdown as shown:</p> <p></p> <p>When you click on \"View code\", you will see output similar to the following which shows all the details of the API call including model name, prompt text, and parameters.</p> <p></p> <p>Now that we have identified the model, parameters, and prompt that we will use, let's move on to Lab 2 where we will build the application using this information.</p>"},{"location":"lab-2/","title":"Create your application which uses LLM to generate information","text":"<p>The goal of this lab is to show how you can leverage Generative AI in an application to bring value to the app. The application we are going to create is a travel chatbot app, and we will use watsonx Assistant to help simplify making the chatbot. For more information on creating chatbots using watsonx Assistant, check out the watsonx chatbot lab. As discussed in Lab 1, we will use a Large Language Model (LLM) to generate the travel information.</p>"},{"location":"lab-2/#steps","title":"Steps","text":""},{"location":"lab-2/#step-1-obtain-the-openapi-definition-of-the-watsonxai-generation-endpoint","title":"Step 1. Obtain the OpenAPI definition of the watsonx.ai generation endpoint","text":"<p>Before you can add the integration between watsonx.ai and watsonx Assistant virtual assistant to enable a LLM to be called from the chatbot, you must use an OpenAPI definition for the watsonx.ai foundation model inferencing service. An example definition file is available in the Assistant toolkit GitHub repository. However, for this workshop, you'll use a version that is modified with additional support for the <code>decode_method</code> parameter of the API. Download this version of the watsonx-openapi.json file to your workstation.</p> <p>Note: <code>servers.url</code> property should be set to the same IBM Cloud region as where your watsonx.ai and watsonx Assistant are deployed to.</p>"},{"location":"lab-2/#step-2-add-an-empty-virtual-assistant-to-the-watsonx-assistant-instance","title":"Step 2. Add an empty virtual assistant to the watsonx Assistant instance","text":"<p>For initial experimentation with the integration, use a new virtual assistant. If you have a newly created watsonx Assistant service instance, it will not have any assistants defined. Create one using the Create Assistant wizard that uses the name <code>Travel App</code>, and then select <code>web</code> as the deployment location.</p> <p>The values that you provide in the other parts of the wizard do not matter. For a slightly opinionated, step-by-step example of how to get a web assistant created, see the Technical Sales watsonx Assistant 101 lab up to the point where you see the home page for the assistant. If you have other assistants defined already in your instance, add one by following the Adding more assistants documentation.</p>"},{"location":"lab-2/#step-3-create-a-watsonxai-api-key","title":"Step 3. Create a watsonx.ai API key","text":"<p>If you have previously created an API key for calling watsonx.ai from a notebook or just the REST endpoint with <code>curl</code>, you can skip these steps and reuse that API key. Otherwise:</p> <ol> <li> <p>Log in to watsonx.ai, and make sure that the same account as the Watson Machine Learning instance for your watsonx.ai project is selected.</p> </li> <li> <p>To generate an API key from your IBM Cloud user account, go to Manage access and users - API Keys, and click Create. In the pop-up window, provide a name like <code>watsonx-apikey</code> and a short description, then click Create.    </p> </li> <li> <p>Immediately make a copy of the API key and store it in a secure location as there is no way to retrieve it again.</p> </li> </ol>"},{"location":"lab-2/#step-4-getting-your-watsonxai-project-id","title":"Step 4. Getting your watsonx.ai project ID","text":"<ol> <li> <p>In the watsonx.ai page, select the left 4-bar pull-down menu, expand Projects, and then click View all projects.</p> </li> <li> <p>From the list, select the project that you are using for the integration.</p> </li> <li> <p>Click the Manage tab, and ensure that General is selected. Copy the Project ID to a location where you can access it later. This is not sensitive like the API key, so it can be in a notepad or something equivalent.</p> </li> </ol> <p></p>"},{"location":"lab-2/#step-5-add-the-custom-extension-for-watsonxai-to-the-integrations-catalog","title":"Step 5. Add the custom extension for watsonx.ai to the Integrations catalog","text":"<ol> <li> <p>Return to your browser tab with the assistant that you created. In the assistant left navigation panel, go down to the lower section and click Integrations.</p> </li> <li> <p>Scroll down to the Extensions section, and click Build custom extension.    </p> </li> <li> <p>Review the getting started guide, and click Next.</p> </li> <li> <p>On the basic information tab, name the extension <code>watsonx</code>, and provide an optional description. Click Next.</p> </li> <li> <p>Either drag the file or use the file browser to select the <code>watsonx-openapi.json</code> file, and then click Next.    </p> </li> <li> <p>Review the information displayed, and click Finish. The integration catalog shows the integration.    </p> </li> <li> <p>Click Add in the lower-right corner of the new integration tile, and confirm Add when prompted.</p> </li> <li> <p>Review the Get Started information, and click Next.</p> </li> <li> <p>In the Authentication panel, select <code>Oauth 2.0</code>, enter the API key that you created earlier, keep the remaining default values, then click Next.    </p> </li> <li> <p>On the last page, review the <code>POST</code> request parameters and response values. The assistant can set parameters in the request before calling the extension and will parse the response to show the user the output. Click Finish, then click Close if you are not returned to the Extensions section of the Integrations panel.</p> </li> </ol> <p>Congratulations, you now have a way to make a functional integration from an Action in an assistant with a watsonx generative AI endpoint. In other words, you now have the means to inference a LLM from a chatbot.</p>"},{"location":"lab-2/#step-6-create-your-action","title":"Step 6. Create your action","text":"<p>Now, it's time to create your action and have it use the integration to watsonx. To get started, you create an action that uses generative AI to generate travel information from your prompt.</p> <ol> <li> <p>In the browser tab with watsonx Assistant, click Actions in the upper left.</p> </li> <li> <p>In the main panel, click Create action.</p> </li> <li> <p>Select Start from Scratch.</p> </li> <li> <p>For what the customer says, enter <code>Travel</code>, and click Save.    </p> </li> <li> <p>Click the pencil icon to set a step title. Use <code>Specify country</code> as the title.</p> </li> <li> <p>For what the Assistant says, you can either make it a short message or be more prescriptive. Humans, can often do better when requests are specific. For example:     \"What country would you like to travel to?\"        Expand Define customer response, and specify that the assistant will expect a response from the user as Free Text input. When you click Free Text, the UI shows User enters free text below the Assistant says panel. When this is shown, click New Step to configure the next part of the Action.</p> </li> <li> <p>Name the next step <code>Information on country</code> by clicking the pencil (edit) icon. In this step, select with conditions. In the Conditions, click the first item, click \"Action step variables\" and select 1. Specify country.    </p> </li> <li> <p>For what the Assistant says, add text <code>Would you like information on</code>, and type <code>$</code>. After you paste this, a pull-down menu appears. Click \"Action step variables\" and select 1. Specify country.        Expand Define customer response, and specify that the assistant will expect a response from the user as Confirmation. When you click Confirmation, the UI shows Yes No below the Assistant says panel. When this is shown, click New Step to configure the next part of the Action.</p> </li> <li> <p>Name the next step <code>No information on country required</code> by clicking the pencil (edit) icon. In this step, select with conditions. In the Conditions, click the first item, click \"Action step variables\" and select 2. Information on country. Click the third item and select No.        Expand And then, and specify that the next step is End the action. When you click End the action, the UI shows End the action in the And then panel. When this is shown, click New Step to configure the next part of the Action. This steps means a user can end the dialogue flow if they don't want information on the country specified.</p> </li> <li> <p>Name the next step <code>Call LLM for general country information</code> by clicking the pencil (edit) icon. In this step, select with conditions. In the Conditions, click the first item, click \"Action step variables\" and select 2. Information on country. Click the third item and select Yes. Click Set variable values, and click Set new value. Then, click New session variable.    </p> </li> <li> <p>Call the variable <code>prompt</code> with type free text, and click Apply.    </p> </li> <li> <p>For the variable assignment <code>To</code>, select Expression, and type <code>\"Tell me about the country \"</code>, then <code>+</code> and finally <code>$</code>. After you paste this, a pull-down menu appears. Click \"Action step variables\" and select 1. Specify country. Click Apply.     </p> </li> <li> <p>In the And then section, select Use an extension.     </p> </li> <li> <p>Select watsonx for the extension to use. Then, select the Generation operation to show the parameters that can be sent to watsonx.ai on every invocation. For version, choose Enter text, and then provide 2023-05-29. For input, choose Session Variables, and then choose prompt, continuing in the same way. Set the <code>model_id</code> to meta-llama/llama-2-70b-chat and the <code>project_id</code> to your watsonx.ai project ID saved previously. Don't click Apply yet.    </p> </li> <li> <p>Expand the optional parameters. For this text generation scenario:</p> <ul> <li>Set <code>parameters.temperature</code> to 0.7</li> <li>Set <code>parameters.max_net tokens</code> to 200</li> <li>Set <code>parameters.min_new_tokens</code> to 50</li> <li>Set <code>parameters.repetition penalty</code> to 2</li> <li>Set <code>parameters.decoding_method</code> to sample</li> <li>Click Apply.    </li> </ul> </li> <li> <p>Click New Step.</p> </li> <li> <p>In this step, name it <code>Country info response</code> and select with conditions. In the condition, click the first item, and select watsonx (step 4).     </p> </li> <li> <p>Select Ran successfully.</p> </li> <li> <p>Click Set variable values, and click Set new value. Then, click New session variable. This is similar to variable set in #10 above.</p> </li> <li> <p>Call the variable <code>result_country_info</code>, select free text, and click Apply. This is similar to #11 above.</p> </li> <li> <p>For the variable assignment <code>To</code>, select Expression, and type <code>$</code>. After you paste this, a pull-down menu appears. Select watsonx (step 4).          Select body.results. The box fills in with the assignment for the <code>result_country_info</code> variable. Now append <code>[0].generated_text</code> to the expression and click Apply.     </p> </li> <li> <p>The full expression for the variable should be displayed, as shown in the following image.     </p> </li> <li> <p>In the Assistant says panel, select function, and then enter <code>${result_country_info}</code>. Or, you can select Session variables, and then result_country_info.      This panel will display the information to the chatbot user as returned by the LLM when inferenced. It uses the <code>result_country_info</code> variable to store the result from the LLM.</p> </li> <li> <p>Click New Step.</p> </li> <li> <p>In this step, name it <code>More info on country</code>, it prompts the user if they would like more specific information on the country as you did in #7 and #8 above. In the Assistant says, add text <code>Would you like more information on</code>, type <code>$</code>. After you paste this, a pull-down menu appears. Click \"Action step variables\" and select 1. Specify country. Next, add text: <code>that is specific to your interests?</code>. In Define customer response, specify that the assistant will expect a response from the user as Confirmation.    </p> </li> <li> <p>Click New Step.</p> </li> <li> <p>Name the next step <code>Specific interests on country</code> by clicking the pencil (edit) icon. In this step, select with conditions. In the condition, click the first item, click \"Action step variables\" and select 6. More info on country. Click the third item and select Yes. In the Assistant says panel, add text <code>Please list your interests</code>. In Define customer response, specify that the assistant will expect a response from the user as Free Text input    </p> </li> <li> <p>Click New Step.</p> </li> <li> <p>As per #9 above, create a step (name it <code>No specific interests on country</code>) which checks if the user said No for more information and End the action in the And then panel.    </p> </li> <li> <p>Click New Step.</p> </li> <li> <p>In this step, using the list of interests from the user, add a <code>prompt_specific</code> session variable that will be used to prompt the model. Click Apply when variable expression complete.    </p> </li> <li> <p>Select watsonx for the extension to use (similar to #14 and #15 above). The only difference in the parameters here is input, where you choose Session Variables, and then choose prompt_specific instead.        Click Apply.</p> </li> <li> <p>Click New Step.</p> </li> <li> <p>In this step (name it <code>Country specific info response</code>), following similar instructions from #17 to #23. Condition is based on <code>watsonx (step 9)</code>. The new variable to create is called <code>result_country_specific_info</code> which is added to Assistant says to output the result to the user. In And then, specify that the next step is End the action as this is the final step.    </p> </li> </ol> <p>The assistant is ready to go. It's time to test it out.</p>"},{"location":"lab-2/#step-7-testing-the-travel-app","title":"Step 7. Testing the travel app","text":"<ol> <li> <p>Click Preview.</p> </li> <li> <p>After the greeting, type <code>travel</code>, and send to the chatbot.</p> </li> <li> <p>The chatbot replies with the prompt for the country and if you want info on it.    </p> </li> <li> <p>Click Yes and if everything worked, you should receive a response from the model containing the info on the country you specified.     You most likely will see something different if you rerun for the same country because you are using a <code>sampling</code> (versus <code>greedy</code>) decoding approach. If you don't like the response, rerun the assistant to get another message.</p> </li> <li> <p>If you click Yes for information more specific to your interests then you will be prompted to enter your interests. Once interests are entered, if everything worked, you should receive a response from the model with more specific info on the country.    </p> </li> </ol>"},{"location":"lab-2/#next-steps","title":"Next Steps","text":"<p>Experiment in the watsonx.ai prompt lab for other scenarios using different prompts and parameters, then create new actions and adjust the parameters for the new use case. Try it out!</p> <p>Continue exploring watsonx Assistant and watsonx.ai to learn more features and functions.</p> <p></p>"},{"location":"pre-work/","title":"Pre-work","text":"<p>This section is broken up into the following steps:</p> <ol> <li>Sign up for IBM Cloud</li> <li>Create a watsonx Assistant instance</li> <li>Connect to watsonx.ai</li> <li>Create a sandbox project in watsonx.ai</li> </ol>"},{"location":"pre-work/#1-sign-up-for-ibm-cloud","title":"1. Sign up for IBM Cloud","text":"<p>Ensure you have an IBM Cloud ID as services and Large Language Models (LLMs) will be deployed on the cloud.</p> <p></p> <p>Note: You will need to be logged into IBM Cloud before continuing below.</p>"},{"location":"pre-work/#2-create-a-watsonx-assistant-instance","title":"2. Create a watsonx Assistant instance","text":"<p>You will use watsonx Assistant to create a dialog flow and integrate an LLM to generate the travel chatbot app.</p> <p>Deploy an instance of watsonx Assistant if you don't have one or want to use a new one. Choose any of the service plans, including the Lite plan:</p> <p></p> <p>Once deployed, launch the Assistant so you can start using it.</p> <p></p>"},{"location":"pre-work/#3-connect-to-watsonxai","title":"3. Connect to watsonx.ai","text":"<p>You will use watsonx.ai for prompt tuning and inferencing AI models in the travel chatbot app, so ensure that you can log into the UI. </p> <p>If you haven't already signed up, click \"Start your free trial\" and follow the instructions. You'll be able to use your free trial for this lab and further self-study, just be mindful of token limits as they vary depending on pricing tier.</p>"},{"location":"pre-work/#4-create-a-sandbox-project-in-watsonxai","title":"4. Create a sandbox project in watsonx.ai","text":"<p>A project is where you work with data and models. When you sign up for watsonx.ai, your sandbox project is created automatically, and you can start working with it immediately. If you don't have a project, you can manually create one too.</p> <p></p> <p>Once you have a project, you'll be able to open the Prompt Lab, where you can experiment with prompting different foundation models, explore sample prompts, and save and share your best prompts.</p> <p></p> <p>You are now ready to move on to the first lab where we'll use prompt engineering of foundation models to better understand what model and input prompt we need to be able to use the model in our application to return the information we require. In the subsequent lab, we will then integrate the learnings from the first lab to call the model in the dialog flow of our travel chatbot application.</p>"}]}